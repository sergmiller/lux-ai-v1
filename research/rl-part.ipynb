{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "import json\n",
    "import os\n",
    "\n",
    "import subprocess\n",
    "\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%config Completer.use_jedi = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "\n",
    "from tensorboardX import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'utils.dataset' from '/Users/sergmiller/Documents/my/lux-ai-v1/research/utils/dataset.py'>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from importlib import reload\n",
    "from utils import dataset\n",
    "\n",
    "reload(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def learn(train, val, model_ff, criterion, iter_data, epochs=5, batch_size=64, shuffle=True, freq=10,lr=1e-3, l2=1e-5, optimizer=None, writer=None): \n",
    "    assert iter_data is not None\n",
    "    if writer is None:\n",
    "        writer = SummaryWriter()\n",
    "    \n",
    "#     np.random.seed(1)\n",
    "    ids_nn = np.arange(train.targets.shape[0])\n",
    "    \n",
    "    reshape_to_last = lambda x: torch.reshape(x, [np.prod(x.shape[:-1]), x.shape[-1]])\n",
    "\n",
    "    if optimizer is None:\n",
    "        optimizer = optim.Adam(model_ff.parameters(), lr=lr, weight_decay=l2)\n",
    "\n",
    "    time_for_print_loss = lambda i: (i + 1) % freq == 0\n",
    "    \n",
    "    n_iter = 0\n",
    "\n",
    "\n",
    "    for epoch in np.arange(epochs):\n",
    "        np.random.shuffle(ids_nn)\n",
    "\n",
    "        model_ff.train(True)\n",
    "\n",
    "        for b in np.arange(0, train.targets.shape[0], batch_size):\n",
    "            X_batch = torch.FloatTensor(train.features[ids_nn[b:b+batch_size]])\n",
    "            y_batch = torch.FloatTensor(train.weights[ids_nn[b:b+batch_size]])  # reward(advantage)\n",
    "            a_batch = torch.LongTensor(train.targets[ids_nn[b:b+batch_size]])  # action\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            y_pred_logits = model_ff(X_batch)\n",
    "\n",
    "            loss = criterion(y_pred_logits, y_batch, a_batch, X_batch, model_ff, (writer, iter_data[\"n_iter\"]))\n",
    "            loss.backward()\n",
    "\n",
    "            optimizer.step()\n",
    "\n",
    "            if (b // batch_size + 1) % freq == 0:\n",
    "                print('train loss in %d epoch in %d batch: %.5f' %\n",
    "                  (epoch + 1, b // batch_size + 1, loss.item()))\n",
    "\n",
    "                writer.add_scalar('data/train_loss', loss.item(), iter_data[\"n_iter\"])\n",
    "                writer.add_scalar('data/epoch', epoch + 1, iter_data[\"n_iter\"])\n",
    "                writer.add_scalar('data/batch', b // batch_size + 1, iter_data[\"n_iter\"])\n",
    "\n",
    "                val_loss = 0\n",
    "                its = 0\n",
    "                model_ff.train(False)\n",
    "                for b in np.arange(0, val.targets.shape[0], batch_size):\n",
    "                    its += 1\n",
    "                    X_batch = torch.FloatTensor(val.features[b:b+batch_size])\n",
    "#                     X_batch = reshape_to_last(X_batch)\n",
    "\n",
    "                    y_batch = torch.FloatTensor(val.weights[b:b+batch_size])\n",
    "                    a_batch = torch.LongTensor(val.targets[b:b+batch_size])\n",
    "                    with torch.no_grad():\n",
    "                        y_pred_logits = model_ff(X_batch)\n",
    "                    loss = criterion(y_pred_logits, y_batch, a_batch, X_batch, model_ff, None)\n",
    "                    val_loss += loss.item()\n",
    "                val_loss /= its\n",
    "                print('val loss in %d epoch: %.5f' % (epoch + 1, val_loss))\n",
    "\n",
    "                writer.add_scalar('data/val_loss', val_loss, iter_data[\"n_iter\"])\n",
    "                n_iter += 1\n",
    "                iter_data[\"n_iter\"] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# datasets = dataset.read_datasets_from_dir(\"features_v3/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset.read_columns_from_random_file(\"features_v3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset.CAT_FEATURES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "FLOAT_FEATURES = [i for i in range(42 + 32*32*7) if i not in dataset.CAT_FEATURES_V4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([0, 1, 5, 6, 7, 9, 10, 11, 12, 13, 15, 17, 18, 19, 20, 21, 23, 26, 27, 28],\n",
       " [7200, 7201, 7202, 7203, 7204, 7205, 7206, 7207, 7208, 7209])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "FLOAT_FEATURES[:20], FLOAT_FEATURES[-10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../submissions/simple/models/ohe_v2\", \"rb\") as f:\n",
    "    OHE = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_features(t: dataset.Dataset, v: dataset.Dataset, ohe=None, categories=None) -> (dataset.Dataset, dataset.Dataset):\n",
    "    create_ohe = ohe is None\n",
    "    if create_ohe:\n",
    "         ohe = OneHotEncoder(sparse=False, categories=categories)\n",
    "    def prepare(d, is_train):\n",
    "        cf = d.features[:, dataset.CAT_FEATURES_V4]\n",
    "        ff = d.features[:, FLOAT_FEATURES]\n",
    "        cf[cf == \"False\"] = False\n",
    "        cf[cf == \"True\"] = True\n",
    "        cf[cf == None] = \"None\"\n",
    "        cf[cf == \"1\"] = 1\n",
    "        cf[cf == \"2\"] = 2\n",
    "        cf[cf == \"3\"] = 3\n",
    "        ff[ff == \"None\"] = 0\n",
    "        cf_o = ohe.fit_transform(cf) if is_train and create_ohe else ohe.transform(cf)\n",
    "        return dataset.Dataset(\n",
    "            features=np.array(np.concatenate([cf_o, ff], axis=1), dtype=np.float),\n",
    "            targets=np.array(d.targets, dtype=np.float),\n",
    "            weights=np.array(d.weights, dtype=np.float),\n",
    "            next_state_id = d.next_state_id\n",
    "        )\n",
    "    t = prepare(t, True)\n",
    "    v = prepare(v, False)\n",
    "    return (t,v, ohe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dt,dv,OHE = prepare_features(data, data, None, [\n",
    "#      np.array([\"None\", False, True], dtype=object),\n",
    "#      np.array([1, 2, 3], dtype=object),\n",
    "#      np.array(['None', 'bcity', 'e', 'n', 'p', 's', 'w'], dtype=object),\n",
    "#      np.array(['None', 'c', 'e', 'n', 's', 'w'], dtype=object),\n",
    "#      np.array([\"None\", False, True], dtype=object),\n",
    "#      np.array(['None', 'e', 'n', 's', 'w'], dtype=object),\n",
    "#      np.array([\"None\", False, True], dtype=object),\n",
    "#      np.array(['None', 'c', 'e', 'n', 's', 'w'], dtype=object),\n",
    "#      np.array(['None', 'coal', 'uranium', 'wood'], dtype=object),\n",
    "#      np.array([\"None\", False, True], dtype=object),\n",
    "#      np.array([\"None\", False, True], dtype=object),\n",
    "#      np.array([\"None\", False, True], dtype=object),\n",
    "#      np.array([\"None\", False, True], dtype=object),\n",
    "#      np.array([\"None\", False, True], dtype=object)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array(['None', False, True], dtype=object),\n",
       " array([1, 2, 3], dtype=object),\n",
       " array(['None', 'bcity', 'e', 'n', 'p', 's', 'w'], dtype=object),\n",
       " array(['None', 'c', 'e', 'n', 's', 'w'], dtype=object),\n",
       " array(['None', False, True], dtype=object),\n",
       " array(['None', 'e', 'n', 's', 'w'], dtype=object),\n",
       " array(['None', False, True], dtype=object),\n",
       " array(['None', 'c', 'e', 'n', 's', 'w'], dtype=object),\n",
       " array(['None', 'coal', 'uranium', 'wood'], dtype=object),\n",
       " array(['None', False, True], dtype=object),\n",
       " array(['None', False, True], dtype=object),\n",
       " array(['None', False, True], dtype=object),\n",
       " array(['None', False, True], dtype=object),\n",
       " array(['None', False, True], dtype=object)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "OHE.categories_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(\"../submissions/simple/models/ohe_v2\", \"wb\") as f:\n",
    "#     pickle.dump(OHE, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(\"../submissions/simple/models/ohe_v1\", \"wb\") as f:\n",
    "#     pickle.dump(ohe, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAP_F = 32 * 32 * 7\n",
    "\n",
    "class NNWithCustomFeatures(nn.Module):\n",
    "    def __init__(self, INPUT_F, DROP_P, H, A=6):\n",
    "        super().__init__()\n",
    "        INPUT_F_C = INPUT_F + 128\n",
    "        self.model_q =  nn.Sequential(\n",
    "            nn.Dropout(DROP_P),\n",
    "            nn.Linear(INPUT_F_C, H),\n",
    "            nn.LayerNorm(H),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(DROP_P),\n",
    "            nn.Linear(H, H),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(DROP_P),\n",
    "            nn.Linear(H, H),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(H, A)\n",
    "        )\n",
    "        \n",
    "        self.model_p =  nn.Sequential(\n",
    "            nn.Dropout(DROP_P),\n",
    "            nn.Linear(INPUT_F_C, H),\n",
    "            nn.LayerNorm(H),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(DROP_P),\n",
    "            nn.Linear(H, H),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(DROP_P),\n",
    "            nn.Linear(H, H),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(H, A)\n",
    "        )\n",
    "\n",
    "        self.map_model = nn.Sequential(\n",
    "            nn.Conv2d(7, 64, 3),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2),  # after -> (16,16)\n",
    "            nn.Conv2d(64, 128, 3),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2),  # after -> (8, 8)\n",
    "            nn.Conv2d(128, 256, 3),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2),  # after -> (4, 4)\n",
    "        )\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((4, 4))\n",
    "        self.proj = nn.Sequential(\n",
    "            nn.Dropout(p=DROP_P),\n",
    "            nn.Linear(256 * 4 * 4, 256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(p=DROP_P),\n",
    "            nn.Linear(256, 128)\n",
    "        )\n",
    "        \n",
    "    #quriosity block\n",
    "            \n",
    "        self.model_encoder = nn.Sequential(\n",
    "            nn.Dropout(DROP_P),\n",
    "            nn.Linear(INPUT_F_C, H),\n",
    "            nn.LayerNorm(H),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(DROP_P),\n",
    "            nn.Linear(H, H),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(DROP_P),\n",
    "            nn.Linear(H, H),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(H, H)\n",
    "        )\n",
    "                \n",
    "        self.forward_model =  nn.Sequential(\n",
    "            nn.Dropout(DROP_P),\n",
    "            nn.Linear(H + A, H),\n",
    "            nn.LayerNorm(H),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(DROP_P),\n",
    "            nn.Linear(H, H),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(DROP_P),\n",
    "            nn.Linear(H, H)\n",
    "        )\n",
    "        \n",
    "        self.inverse_model =  nn.Sequential(\n",
    "            nn.Dropout(DROP_P),\n",
    "            nn.Linear(H * 2, H),\n",
    "            nn.LayerNorm(H),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(DROP_P),\n",
    "            nn.Linear(H, H),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(DROP_P),\n",
    "            nn.Linear(H, A)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        L = x.shape[1]\n",
    "        cur_r = self.forward_impl(x[:, :L // 2])\n",
    "        next_r =  self.forward_impl(x[:, L // 2:])\n",
    "        return torch.cat([cur_r, next_r],dim=1)\n",
    "\n",
    "    def forward_impl(self, x):\n",
    "        mapp = x[:, -MAP_F:].reshape(-1, 32, 32, 7)\n",
    "        rest = x[:, :-MAP_F]\n",
    "        mapp = torch.transpose(mapp, 1, -1)\n",
    "        mapp = self.avgpool(self.map_model(mapp))\n",
    "        mapp = torch.flatten(mapp, 1)\n",
    "        mapp_f = self.proj(mapp)\n",
    "        input_x = torch.cat([rest, mapp_f], dim=1)\n",
    "        return torch.cat([self.model_q(input_x), self.model_p(input_x)], dim=1)\n",
    "\n",
    "    def phi(self, x):\n",
    "        mapp = x[:, -MAP_F:].reshape(-1, 32, 32, 7)\n",
    "        rest = x[:, :-MAP_F]\n",
    "        mapp = torch.transpose(mapp, 1, -1)\n",
    "        mapp = self.avgpool(self.map_model(mapp))\n",
    "        mapp = torch.flatten(mapp, 1)\n",
    "        mapp_f = self.proj(mapp)\n",
    "        input_x = torch.cat([rest, mapp_f], dim=1)\n",
    "        return self.model_encoder(input_x)\n",
    "\n",
    "    def calc_curiosity_reward_and_restore_action(self, x, a_batch):\n",
    "        L = x.shape[1]\n",
    "        cur_s = x[:, :L // 2]\n",
    "        next_s = x[:, L // 2:]\n",
    "        phi_cur = self.phi(cur_s)\n",
    "        phi_next = self.phi(next_s)\n",
    "        forward_input = torch.cat([phi_cur, F.one_hot(a_batch, 6)], dim=1)\n",
    "        phi_hat_next = self.forward_model(forward_input)\n",
    "        reward = torch.mean((phi_hat_next - phi_next) ** 2)\n",
    "        phi_cur_and_next = torch.cat([phi_cur, phi_next], dim=1)\n",
    "        inv_action_pred = self.inverse_model(phi_cur_and_next)\n",
    "        inv_loss = torch.mean(torch.nn.CrossEntropyLoss(reduction='none')(inv_action_pred, a_batch))\n",
    "        return reward, inv_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'torch.version' from '/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/torch/version.py'>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NNWithCustomFeatures(63, 0.05, 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0459, -0.2247,  0.0816, -0.0187, -0.2221,  0.0039,  0.0616, -0.1225,\n",
       "          0.1290,  0.0431,  0.0778,  0.0308],\n",
       "        [-0.0899, -0.1996,  0.0961, -0.0575, -0.2508,  0.0254,  0.1081, -0.0994,\n",
       "          0.1516,  0.0927,  0.0568,  0.0088],\n",
       "        [-0.1035, -0.1845,  0.1133, -0.0828, -0.2799, -0.0075,  0.0454, -0.1296,\n",
       "          0.1411,  0.0249,  0.1404,  0.0361],\n",
       "        [-0.1028, -0.1537,  0.1107, -0.0798, -0.2707, -0.0121,  0.0728, -0.1136,\n",
       "          0.1850,  0.0318,  0.0977,  0.0129]], grad_fn=<CatBackward0>)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.forward_impl(torch.Tensor(4, 63 + 32*32*7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.0470, grad_fn=<MeanBackward0>),\n",
       " tensor(1.9528, grad_fn=<MeanBackward0>))"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.calc_curiosity_reward_and_restore_action(torch.Tensor(4, 2 * (63 + 32*32*7)), torch.LongTensor([1,2,3,4]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "ENTROPY_REG = 1.0\n",
    "PI_REG = 1.0\n",
    "CURIOSITY_WEIGTH = 1\n",
    "CURIOSITY_BETA = 0.1\n",
    "def policy_loss(pi_logits, reward_batch, a_batch, X_batch, _):\n",
    "    pi_probs = torch.nn.Softmax(dim=1)(pi_logits)\n",
    "    return torch.mean(torch.nn.CrossEntropyLoss(reduction='none')(pi_logits, a_batch) * reward_batch \n",
    "                      - torch.sum(pi_probs * torch.log(pi_probs) * ENTROPY_REG, dim=1))\n",
    "\n",
    "def q_loss(q_vals, reward_batch, a_batch, X_batch, _):\n",
    "    q_vals_per_reward = q_vals[np.arange(q_vals.shape[0]), a_batch]\n",
    "    return torch.nn.MSELoss()(q_vals_per_reward, reward_batch) * 0.01\n",
    "\n",
    "gamma = 0.99\n",
    "\n",
    "def get_is_last_state(x):\n",
    "    t = torch.sum(torch.isclose(x, torch.ones_like(x) * (-1)), dim=1) == x.shape[1]\n",
    "    return t.float()\n",
    "\n",
    "def q_loss_pair(q_vals_cur_and_next, reward_batch, a_batch, X_batch, _):\n",
    "    q_vals = q_vals_cur_and_next[:, :6]\n",
    "    q_vals_next = q_vals_cur_and_next[:, 6:12]\n",
    "    q_vals_per_reward_cur = q_vals[np.arange(q_vals.shape[0]), a_batch]\n",
    "    X_batch_next = X_batch[:, X_batch.shape[1] // 2:]\n",
    "    best_q_vals_next = torch.max(q_vals_next,dim=1)[0] * (1 - get_is_last_state(X_batch_next))\n",
    "#     print(list(enumerate([q_vals_per_reward_cur, reward_batch, best_q_vals_next, q_vals_next])))\n",
    "#     print(0.99 * best_q_vals_next)\n",
    "#     return torch.nn.MSELoss()(target=q_vals_per_reward_cur.detach(), input=reward_batch + gamma * best_q_vals_next)\n",
    "    return torch.nn.SmoothL1Loss()(target=q_vals_per_reward_cur.detach(), input=reward_batch + gamma * best_q_vals_next)\n",
    "\n",
    "\n",
    "def actor_critic_loss(q_pi_payload, _reward_batch, a_batch, X_batch, model_ff, writer_and_iter):\n",
    "#     curiosity_reward_batch = torch.zeros_like(_reward_batch)\n",
    "    curiosity_reward_batch, inv_loss = model_ff.calc_curiosity_reward_and_restore_action(X_batch, a_batch)\n",
    "    reward_batch = _reward_batch + curiosity_reward_batch\n",
    "    q_vals = q_pi_payload[:, :6]\n",
    "    pi_logits =  q_pi_payload[:, 6:12]\n",
    "    pi_probs = torch.nn.Softmax(dim=1)(pi_logits)\n",
    "    q_vals_next = q_pi_payload[:, 12:18]\n",
    "    q_vals_per_reward_cur = q_vals[np.arange(q_vals.shape[0]), a_batch]\n",
    "    X_batch_next = X_batch[:, X_batch.shape[1] // 2:]\n",
    "    best_q_vals_next = torch.max(q_vals_next,dim=1)[0] * (1 - get_is_last_state(X_batch_next))\n",
    "    with torch.no_grad():\n",
    "        reference_q_val = reward_batch + gamma * best_q_vals_next\n",
    "        advantage = reference_q_val - q_vals_per_reward_cur\n",
    "    q_loss = torch.nn.SmoothL1Loss()(target=reference_q_val.detach(), input=q_vals_per_reward_cur)\n",
    "    pi_loss =  torch.mean(torch.nn.CrossEntropyLoss(reduction='none')(pi_logits, a_batch) * advantage.detach()) * PI_REG\n",
    "    entropy = torch.mean(torch.sum(pi_probs * torch.log(pi_probs), dim=1)) * ENTROPY_REG\n",
    "    if writer_and_iter is not None:\n",
    "        writer, n_iter = writer_and_iter\n",
    "        writer.add_scalar('data/batch_reward', torch.mean(_reward_batch).item(), n_iter)\n",
    "        writer.add_scalar('data/curiosity_reward', torch.mean(curiosity_reward_batch).item(), n_iter)\n",
    "        writer.add_scalar('data/train_q_loss', q_loss.item(), n_iter)\n",
    "        writer.add_scalar('data/train_pi_loss', pi_loss.item(), n_iter)\n",
    "        writer.add_scalar('data/train_entropy_loss', entropy.item(), n_iter)\n",
    "    return q_loss + pi_loss + entropy + CURIOSITY_WEIGTH * (CURIOSITY_BETA * inv_loss + (1 - CURIOSITY_BETA) * curiosity_reward_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_bot = \"../submissions/simple/main.py\"\n",
    "replays = \"replays\"\n",
    "\n",
    "def run_game(left_bot=simple_bot, right_bot=simple_bot, seed=42, loglevel=2):\n",
    "    replay_path = \"replay.json\"\n",
    "    python_v = \"python3.7\"\n",
    "    \n",
    "    replay_path = os.path.join(replays, str(np.random.randint(1e9)) + \".json\")\n",
    "    \n",
    "    size = np.random.choice([12,16,24,32], size=1)[0]\n",
    "    \n",
    "    res = subprocess.run([\n",
    "        \"lux-ai-2021\",\n",
    "        left_bot,\n",
    "        right_bot,\n",
    "#         \"--statefulReplay\",\n",
    "        \"--width={}\".format(size),\n",
    "        \"--height={}\".format(size),\n",
    "        \"--loglevel={}\".format(loglevel),\n",
    "        \"--python={}\".format(python_v),\n",
    "        \"--maxtime=100000\",\n",
    "        \"--maxConcurrentMatches=4\",\n",
    "        \"--seed={}\".format(seed),\n",
    "        \"--out={}\".format(replay_path)], stdout=subprocess.PIPE)\n",
    "    \n",
    "    if loglevel > 0:\n",
    "        print(res.stdout.decode())\n",
    "\n",
    "    assert res.returncode == 0\n",
    "\n",
    "    with open(replay_path, \"r\") as f:\n",
    "        result = json.load(f)\n",
    "    return result, res.stdout.decode()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 432,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "run_game(simple_bot, simple_bot)  # <-- test run one game with default bot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hashlib\n",
    "\n",
    "def build_runnable_bot_with_flags(flags: dict, origin = simple_bot, base_path = '../submissions/simple/') -> str:\n",
    "    lines = []\n",
    "    with open(origin, \"r\") as f:\n",
    "        for line in f:\n",
    "            lines.append(line[:-1])\n",
    "    text = '\\n'.join(lines)\n",
    "    f = json.dumps(flags)\n",
    "    text = text.format(f)\n",
    "    h = int(hashlib.sha256(f.encode('utf-8')).hexdigest(), 16) % (10 ** 18)\n",
    "    path = base_path + \"main_\" + str(h) + \".py\"\n",
    "    with open(path, \"w\") as f:\n",
    "        f.write(text)\n",
    "    return path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_series(results: list):\n",
    "    wins = []\n",
    "    for i, r in enumerate(results):\n",
    "        ranks = r[0]['results']['ranks']\n",
    "        teams = r[0]['teamDetails']\n",
    "        if ranks[0]['rank'] == 1 and ranks[1]['rank'] == 2:\n",
    "            if ranks[0][\"agentID\"] == i % 2:\n",
    "                wins.append(1)\n",
    "            else:\n",
    "                wins.append(0)\n",
    "        else:\n",
    "            wins.append(0.5)\n",
    "    return wins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib import Parallel, delayed\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_dataset(d, p=0.5):\n",
    "    N = len(d.features)\n",
    "    ids = np.random.choice(N, size=int(N * p))\n",
    "    return dataset.Dataset(features = d.features[ids], weights = d.weights[ids], targets = d.targets[ids])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_next_features(d):\n",
    "    assert d.next_state_id is not None\n",
    "    coupled_features = []\n",
    "    weights = []\n",
    "    targets = []\n",
    "    for i in np.arange(d.features.shape[0]):\n",
    "        next_i = d.next_state_id[i]\n",
    "        if d.next_state_id[i] != -1:\n",
    "            next_f = d.features[next_i]\n",
    "        else:\n",
    "            next_f = np.ones_like(d.features[i]) * (-1)\n",
    "        coupled_features.append(np.concatenate([d.features[i], next_f]))\n",
    "        weights.append(d.weights[i])\n",
    "        targets.append(d.targets[i])\n",
    "    return dataset.Dataset(\n",
    "        features=np.array(coupled_features),\n",
    "        weights=np.array(weights),\n",
    "        targets=np.array(targets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Round 1, game: 0, is_win: 0.0, max_step: 69, reward: -0.9999999995, example: {'ranks': [{'rank': 1, 'agentID': 1}, {'rank': 2, 'agentID': 0}], 'replayFile': 'replays/717354021.json'}\n",
      "Round 1, game: 1, is_win: 0.0, max_step: 115, reward: -0.9999999989999999, example: {'ranks': [{'rank': 1, 'agentID': 0}, {'rank': 2, 'agentID': 1}], 'replayFile': 'replays/224766667.json'}\n",
      "Round 1, game: 2, is_win: 0.0, max_step: 158, reward: -0.9999999996666666, example: {'ranks': [{'rank': 1, 'agentID': 1}, {'rank': 2, 'agentID': 0}], 'replayFile': 'replays/592322119.json'}\n",
      "Round 1, game: 3, is_win: 0.0, max_step: 110, reward: -0.99999999975, example: {'ranks': [{'rank': 1, 'agentID': 0}, {'rank': 2, 'agentID': 1}], 'replayFile': 'replays/732152370.json'}\n",
      "Round 1, game: 4, is_win: 0.0, max_step: 109, reward: -0.9999999989999999, example: {'ranks': [{'rank': 1, 'agentID': 1}, {'rank': 2, 'agentID': 0}], 'replayFile': 'replays/542837249.json'}\n",
      "Round 1, game: 5, is_win: 0.0, max_step: 118, reward: -0.9999999995, example: {'ranks': [{'rank': 1, 'agentID': 0}, {'rank': 2, 'agentID': 1}], 'replayFile': 'replays/729416111.json'}\n",
      "Round 1, game: 6, is_win: 0.0, max_step: 111, reward: -0.9999999998333333, example: {'ranks': [{'rank': 1, 'agentID': 1}, {'rank': 2, 'agentID': 0}], 'replayFile': 'replays/170208024.json'}\n",
      "Round 1, game: 7, is_win: 0.0, max_step: 71, reward: -0.9999999995, example: {'ranks': [{'rank': 1, 'agentID': 0}, {'rank': 2, 'agentID': 1}], 'replayFile': 'replays/208285721.json'}\n",
      "train loss in 1 epoch in 1 batch: -1.39229\n",
      "val loss in 1 epoch: -1.45366\n",
      "train loss in 2 epoch in 1 batch: -1.42119\n",
      "val loss in 2 epoch: -1.43947\n",
      "train loss in 3 epoch in 1 batch: -1.41614\n",
      "val loss in 3 epoch: -1.42424\n",
      "train loss in 4 epoch in 1 batch: -1.41845\n",
      "val loss in 4 epoch: -1.44301\n",
      "train loss in 5 epoch in 1 batch: -1.43195\n",
      "val loss in 5 epoch: -1.46359\n",
      "train loss in 6 epoch in 1 batch: -1.46302\n",
      "val loss in 6 epoch: -1.47636\n",
      "train loss in 7 epoch in 1 batch: -1.45643\n",
      "val loss in 7 epoch: -1.48159\n",
      "train loss in 8 epoch in 1 batch: -1.46197\n",
      "val loss in 8 epoch: -1.49218\n",
      "Round 2, game: 0, is_win: 0.0, max_step: 70, reward: -0.9999999995, example: {'ranks': [{'rank': 1, 'agentID': 1}, {'rank': 2, 'agentID': 0}], 'replayFile': 'replays/798842024.json'}\n",
      "Round 2, game: 1, is_win: 0.0, max_step: 229, reward: -0.9999999989999999, example: {'ranks': [{'rank': 1, 'agentID': 0}, {'rank': 2, 'agentID': 1}], 'replayFile': 'replays/731724136.json'}\n",
      "Round 2, game: 2, is_win: 0.0, max_step: 113, reward: -0.9999999996666666, example: {'ranks': [{'rank': 1, 'agentID': 1}, {'rank': 2, 'agentID': 0}], 'replayFile': 'replays/125164325.json'}\n",
      "Round 2, game: 3, is_win: 0.0, max_step: 154, reward: -0.9999999989999999, example: {'ranks': [{'rank': 1, 'agentID': 0}, {'rank': 2, 'agentID': 1}], 'replayFile': 'replays/22531780.json'}\n",
      "Round 2, game: 4, is_win: 0.0, max_step: 70, reward: -0.9999999989999999, example: {'ranks': [{'rank': 1, 'agentID': 1}, {'rank': 2, 'agentID': 0}], 'replayFile': 'replays/946207951.json'}\n",
      "Round 2, game: 5, is_win: 0.0, max_step: 70, reward: -0.9999999995, example: {'ranks': [{'rank': 1, 'agentID': 0}, {'rank': 2, 'agentID': 1}], 'replayFile': 'replays/700345079.json'}\n",
      "Round 2, game: 6, is_win: 0.0, max_step: 195, reward: -0.9999999995, example: {'ranks': [{'rank': 1, 'agentID': 1}, {'rank': 2, 'agentID': 0}], 'replayFile': 'replays/933167200.json'}\n",
      "Round 2, game: 7, is_win: 0.0, max_step: 72, reward: -0.9999999995, example: {'ranks': [{'rank': 1, 'agentID': 0}, {'rank': 2, 'agentID': 1}], 'replayFile': 'replays/313037124.json'}\n",
      "train loss in 1 epoch in 1 batch: -1.42786\n",
      "val loss in 1 epoch: -1.43416\n",
      "train loss in 2 epoch in 1 batch: -1.40636\n",
      "val loss in 2 epoch: -1.43914\n",
      "train loss in 3 epoch in 1 batch: -1.43710\n",
      "val loss in 3 epoch: -1.44361\n",
      "train loss in 4 epoch in 1 batch: -1.42488\n",
      "val loss in 4 epoch: -1.44651\n",
      "train loss in 5 epoch in 1 batch: -1.45070\n",
      "val loss in 5 epoch: -1.44865\n",
      "train loss in 6 epoch in 1 batch: -1.46600\n",
      "val loss in 6 epoch: -1.45145\n",
      "train loss in 7 epoch in 1 batch: -1.43530\n",
      "val loss in 7 epoch: -1.45320\n",
      "train loss in 8 epoch in 1 batch: -1.44661\n",
      "val loss in 8 epoch: -1.45518\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/IPython/core/interactiveshell.py:3338: DtypeWarning: Columns (26,27,28) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  if (await self.run_code(code, result,  async_=asy)):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Round 3, game: 0, is_win: 0.0, max_step: 234, reward: -0.9999999996666666, example: {'ranks': [{'rank': 1, 'agentID': 1}, {'rank': 2, 'agentID': 0}], 'replayFile': 'replays/218175338.json'}\n",
      "Round 3, game: 1, is_win: 0.0, max_step: 70, reward: -0.9999999989999999, example: {'ranks': [{'rank': 1, 'agentID': 0}, {'rank': 2, 'agentID': 1}], 'replayFile': 'replays/446268890.json'}\n",
      "Round 3, game: 2, is_win: 0.0, max_step: 239, reward: -0.9999999995, example: {'ranks': [{'rank': 1, 'agentID': 1}, {'rank': 2, 'agentID': 0}], 'replayFile': 'replays/39738949.json'}\n",
      "Round 3, game: 3, is_win: 0.0, max_step: 30, reward: -0.9999999995, example: {'ranks': [{'rank': 1, 'agentID': 0}, {'rank': 2, 'agentID': 1}], 'replayFile': 'replays/800511694.json'}\n",
      "Round 3, game: 4, is_win: 0.0, max_step: 69, reward: -0.9999999989999999, example: {'ranks': [{'rank': 1, 'agentID': 1}, {'rank': 2, 'agentID': 0}], 'replayFile': 'replays/740382262.json'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/IPython/core/interactiveshell.py:3338: DtypeWarning: Columns (26,27,28) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  if (await self.run_code(code, result,  async_=asy)):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Round 3, game: 5, is_win: 0.0, max_step: 311, reward: -0.9999999998, example: {'ranks': [{'rank': 1, 'agentID': 0}, {'rank': 2, 'agentID': 1}], 'replayFile': 'replays/159508645.json'}\n",
      "Round 3, game: 6, is_win: 0.0, max_step: 110, reward: -0.9999999995, example: {'ranks': [{'rank': 1, 'agentID': 1}, {'rank': 2, 'agentID': 0}], 'replayFile': 'replays/864914992.json'}\n",
      "Round 3, game: 7, is_win: 0.0, max_step: 229, reward: -0.9999999998, example: {'ranks': [{'rank': 1, 'agentID': 0}, {'rank': 2, 'agentID': 1}], 'replayFile': 'replays/890484912.json'}\n",
      "train loss in 1 epoch in 1 batch: -1.47844\n",
      "val loss in 1 epoch: -1.41544\n",
      "train loss in 1 epoch in 2 batch: -1.33407\n",
      "val loss in 1 epoch: -1.43045\n",
      "train loss in 2 epoch in 1 batch: -1.42573\n",
      "val loss in 2 epoch: -1.43713\n",
      "train loss in 2 epoch in 2 batch: -1.51668\n",
      "val loss in 2 epoch: -1.44076\n",
      "train loss in 3 epoch in 1 batch: -1.45928\n",
      "val loss in 3 epoch: -1.44504\n",
      "train loss in 3 epoch in 2 batch: -1.51803\n",
      "val loss in 3 epoch: -1.44872\n",
      "train loss in 4 epoch in 1 batch: -1.50239\n",
      "val loss in 4 epoch: -1.45257\n",
      "train loss in 4 epoch in 2 batch: -1.39548\n",
      "val loss in 4 epoch: -1.45954\n",
      "train loss in 5 epoch in 1 batch: -1.48297\n",
      "val loss in 5 epoch: -1.46417\n",
      "train loss in 5 epoch in 2 batch: -1.50108\n",
      "val loss in 5 epoch: -1.46674\n",
      "train loss in 6 epoch in 1 batch: -1.45606\n",
      "val loss in 6 epoch: -1.47017\n",
      "train loss in 6 epoch in 2 batch: -1.54524\n",
      "val loss in 6 epoch: -1.47090\n",
      "train loss in 7 epoch in 1 batch: -1.49956\n",
      "val loss in 7 epoch: -1.47105\n",
      "train loss in 7 epoch in 2 batch: -1.43759\n",
      "val loss in 7 epoch: -1.47297\n",
      "train loss in 8 epoch in 1 batch: -1.51930\n",
      "val loss in 8 epoch: -1.47537\n",
      "train loss in 8 epoch in 2 batch: -1.47104\n",
      "val loss in 8 epoch: -1.48127\n",
      "Round 4, game: 0, is_win: 0.0, max_step: 190, reward: -0.9999999989999999, example: {'ranks': [{'rank': 1, 'agentID': 1}, {'rank': 2, 'agentID': 0}], 'replayFile': 'replays/932136058.json'}\n",
      "Round 4, game: 1, is_win: 0.0, max_step: 110, reward: -0.99999999975, example: {'ranks': [{'rank': 1, 'agentID': 0}, {'rank': 2, 'agentID': 1}], 'replayFile': 'replays/12809143.json'}\n",
      "Round 4, game: 2, is_win: 0.0, max_step: 32, reward: -0.9999999995, example: {'ranks': [{'rank': 1, 'agentID': 1}, {'rank': 2, 'agentID': 0}], 'replayFile': 'replays/703697079.json'}\n",
      "Round 4, game: 3, is_win: 0.0, max_step: 159, reward: -0.9999999995, example: {'ranks': [{'rank': 1, 'agentID': 0}, {'rank': 2, 'agentID': 1}], 'replayFile': 'replays/38594982.json'}\n",
      "Round 4, game: 4, is_win: 0.0, max_step: 29, reward: -0.9999999989999999, example: {'ranks': [{'rank': 1, 'agentID': 1}, {'rank': 2, 'agentID': 0}], 'replayFile': 'replays/170855354.json'}\n",
      "Round 4, game: 5, is_win: 0.0, max_step: 109, reward: -0.9999999995, example: {'ranks': [{'rank': 1, 'agentID': 0}, {'rank': 2, 'agentID': 1}], 'replayFile': 'replays/499458728.json'}\n",
      "Round 4, game: 6, is_win: 0.0, max_step: 29, reward: -0.9999999989999999, example: {'ranks': [{'rank': 1, 'agentID': 1}, {'rank': 2, 'agentID': 0}], 'replayFile': 'replays/721435036.json'}\n",
      "Round 4, game: 7, is_win: 0.0, max_step: 69, reward: -0.9999999996666666, example: {'ranks': [{'rank': 1, 'agentID': 0}, {'rank': 2, 'agentID': 1}], 'replayFile': 'replays/401700211.json'}\n",
      "train loss in 1 epoch in 1 batch: -1.41854\n",
      "val loss in 1 epoch: -1.44685\n",
      "train loss in 2 epoch in 1 batch: -1.42176\n",
      "val loss in 2 epoch: -1.46658\n",
      "train loss in 3 epoch in 1 batch: -1.44601\n",
      "val loss in 3 epoch: -1.47818\n",
      "train loss in 4 epoch in 1 batch: -1.47333\n",
      "val loss in 4 epoch: -1.48447\n",
      "train loss in 5 epoch in 1 batch: -1.48391\n",
      "val loss in 5 epoch: -1.48992\n",
      "train loss in 6 epoch in 1 batch: -1.48110\n",
      "val loss in 6 epoch: -1.49688\n",
      "train loss in 7 epoch in 1 batch: -1.46558\n",
      "val loss in 7 epoch: -1.50355\n",
      "train loss in 8 epoch in 1 batch: -1.51468\n",
      "val loss in 8 epoch: -1.51271\n",
      "Round 5, game: 0, is_win: 0.0, max_step: 155, reward: -0.999999999875, example: {'ranks': [{'rank': 1, 'agentID': 1}, {'rank': 2, 'agentID': 0}], 'replayFile': 'replays/953453411.json'}\n",
      "Round 5, game: 1, is_win: 0.0, max_step: 189, reward: -0.99999999975, example: {'ranks': [{'rank': 1, 'agentID': 0}, {'rank': 2, 'agentID': 1}], 'replayFile': 'replays/511510674.json'}\n",
      "Round 5, game: 2, is_win: 0.0, max_step: 32, reward: -0.9999999995, example: {'ranks': [{'rank': 1, 'agentID': 1}, {'rank': 2, 'agentID': 0}], 'replayFile': 'replays/328798010.json'}\n",
      "Round 5, game: 3, is_win: 0.0, max_step: 70, reward: -0.9999999995, example: {'ranks': [{'rank': 1, 'agentID': 0}, {'rank': 2, 'agentID': 1}], 'replayFile': 'replays/619180932.json'}\n",
      "Round 5, game: 4, is_win: 0.0, max_step: 150, reward: -0.9999999995, example: {'ranks': [{'rank': 1, 'agentID': 1}, {'rank': 2, 'agentID': 0}], 'replayFile': 'replays/388903682.json'}\n",
      "Round 5, game: 5, is_win: 0.0, max_step: 30, reward: -0.9999999995, example: {'ranks': [{'rank': 1, 'agentID': 0}, {'rank': 2, 'agentID': 1}], 'replayFile': 'replays/820043243.json'}\n",
      "Round 5, game: 6, is_win: 0.0, max_step: 30, reward: -0.9999999989999999, example: {'ranks': [{'rank': 1, 'agentID': 1}, {'rank': 2, 'agentID': 0}], 'replayFile': 'replays/24631899.json'}\n",
      "Round 5, game: 7, is_win: 0.0, max_step: 30, reward: -0.9999999989999999, example: {'ranks': [{'rank': 1, 'agentID': 0}, {'rank': 2, 'agentID': 1}], 'replayFile': 'replays/225472413.json'}\n",
      "train loss in 1 epoch in 1 batch: -1.59317\n",
      "val loss in 1 epoch: -1.63067\n",
      "train loss in 2 epoch in 1 batch: -1.61751\n",
      "val loss in 2 epoch: -1.65811\n",
      "train loss in 3 epoch in 1 batch: -1.61664\n",
      "val loss in 3 epoch: -1.64990\n",
      "train loss in 4 epoch in 1 batch: -1.60794\n",
      "val loss in 4 epoch: -1.63948\n",
      "train loss in 5 epoch in 1 batch: -1.63643\n",
      "val loss in 5 epoch: -1.62988\n",
      "train loss in 6 epoch in 1 batch: -1.60568\n",
      "val loss in 6 epoch: -1.61677\n",
      "train loss in 7 epoch in 1 batch: -1.58222\n",
      "val loss in 7 epoch: -1.60926\n",
      "train loss in 8 epoch in 1 batch: -1.59136\n",
      "val loss in 8 epoch: -1.60570\n",
      "Round 6, game: 0, is_win: 1.0, max_step: 113, reward: 0.9999999998333333, example: {'ranks': [{'rank': 1, 'agentID': 0}, {'rank': 2, 'agentID': 1}], 'replayFile': 'replays/613579658.json'}\n",
      "Round 6, game: 1, is_win: 0.0, max_step: 30, reward: -0.9999999995, example: {'ranks': [{'rank': 1, 'agentID': 0}, {'rank': 2, 'agentID': 1}], 'replayFile': 'replays/21530529.json'}\n",
      "Round 6, game: 2, is_win: 0.0, max_step: 30, reward: -0.9999999989999999, example: {'ranks': [{'rank': 1, 'agentID': 1}, {'rank': 2, 'agentID': 0}], 'replayFile': 'replays/338695131.json'}\n",
      "Round 6, game: 3, is_win: 0.0, max_step: 70, reward: -0.9999999989999999, example: {'ranks': [{'rank': 1, 'agentID': 0}, {'rank': 2, 'agentID': 1}], 'replayFile': 'replays/963484549.json'}\n",
      "Round 6, game: 4, is_win: 0.0, max_step: 69, reward: -0.9999999995, example: {'ranks': [{'rank': 1, 'agentID': 1}, {'rank': 2, 'agentID': 0}], 'replayFile': 'replays/700942678.json'}\n",
      "Round 6, game: 5, is_win: 0.0, max_step: 69, reward: -0.9999999989999999, example: {'ranks': [{'rank': 1, 'agentID': 0}, {'rank': 2, 'agentID': 1}], 'replayFile': 'replays/939039806.json'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/IPython/core/interactiveshell.py:3338: DtypeWarning: Columns (26,27,28) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  if (await self.run_code(code, result,  async_=asy)):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Round 6, game: 6, is_win: 1.0, max_step: 154, reward: 0.9999999998333333, example: {'ranks': [{'rank': 1, 'agentID': 0}, {'rank': 2, 'agentID': 1}], 'replayFile': 'replays/826826325.json'}\n",
      "Round 6, game: 7, is_win: 1.0, max_step: 119, reward: 0.9999999999, example: {'ranks': [{'rank': 1, 'agentID': 1}, {'rank': 2, 'agentID': 0}], 'replayFile': 'replays/15608344.json'}\n",
      "train loss in 1 epoch in 1 batch: -1.41086\n",
      "val loss in 1 epoch: -1.52527\n",
      "train loss in 1 epoch in 2 batch: -1.53956\n",
      "val loss in 1 epoch: -1.54257\n",
      "train loss in 2 epoch in 1 batch: -1.45815\n",
      "val loss in 2 epoch: -1.55034\n",
      "train loss in 2 epoch in 2 batch: -1.56252\n",
      "val loss in 2 epoch: -1.56179\n",
      "train loss in 3 epoch in 1 batch: -1.48381\n",
      "val loss in 3 epoch: -1.56437\n",
      "train loss in 3 epoch in 2 batch: -1.57120\n",
      "val loss in 3 epoch: -1.56550\n",
      "train loss in 4 epoch in 1 batch: -1.49603\n",
      "val loss in 4 epoch: -1.56791\n",
      "train loss in 4 epoch in 2 batch: -1.56815\n",
      "val loss in 4 epoch: -1.57433\n",
      "train loss in 5 epoch in 1 batch: -1.52079\n",
      "val loss in 5 epoch: -1.58283\n",
      "train loss in 5 epoch in 2 batch: -1.57185\n",
      "val loss in 5 epoch: -1.57276\n",
      "train loss in 6 epoch in 1 batch: -1.51032\n",
      "val loss in 6 epoch: -1.55967\n",
      "train loss in 6 epoch in 2 batch: -1.58438\n",
      "val loss in 6 epoch: -1.54922\n",
      "train loss in 7 epoch in 1 batch: -1.49809\n",
      "val loss in 7 epoch: -1.53878\n",
      "train loss in 7 epoch in 2 batch: -1.56583\n",
      "val loss in 7 epoch: -1.53284\n",
      "train loss in 8 epoch in 1 batch: -1.50810\n",
      "val loss in 8 epoch: -1.52740\n",
      "train loss in 8 epoch in 2 batch: -1.53273\n",
      "val loss in 8 epoch: -1.52429\n",
      "Round 7, game: 0, is_win: 0.0, max_step: 110, reward: -0.99999999975, example: {'ranks': [{'rank': 1, 'agentID': 1}, {'rank': 2, 'agentID': 0}], 'replayFile': 'replays/327741615.json'}\n",
      "Round 7, game: 1, is_win: 0.0, max_step: 69, reward: -0.9999999995, example: {'ranks': [{'rank': 1, 'agentID': 0}, {'rank': 2, 'agentID': 1}], 'replayFile': 'replays/882980455.json'}\n",
      "Round 7, game: 2, is_win: 0.0, max_step: 190, reward: -0.9999999995, example: {'ranks': [{'rank': 1, 'agentID': 1}, {'rank': 2, 'agentID': 0}], 'replayFile': 'replays/727448106.json'}\n",
      "Round 7, game: 3, is_win: 0.0, max_step: 155, reward: -0.9999999995, example: {'ranks': [{'rank': 1, 'agentID': 0}, {'rank': 2, 'agentID': 1}], 'replayFile': 'replays/685448979.json'}\n",
      "Round 7, game: 4, is_win: 0.0, max_step: 70, reward: -0.9999999989999999, example: {'ranks': [{'rank': 1, 'agentID': 1}, {'rank': 2, 'agentID': 0}], 'replayFile': 'replays/435056930.json'}\n",
      "Round 7, game: 5, is_win: 0.0, max_step: 109, reward: -0.9999999989999999, example: {'ranks': [{'rank': 1, 'agentID': 0}, {'rank': 2, 'agentID': 1}], 'replayFile': 'replays/805861870.json'}\n",
      "Round 7, game: 6, is_win: 0.0, max_step: 77, reward: -0.9999999996666666, example: {'ranks': [{'rank': 1, 'agentID': 1}, {'rank': 2, 'agentID': 0}], 'replayFile': 'replays/725906352.json'}\n",
      "Round 7, game: 7, is_win: 0.0, max_step: 76, reward: -0.99999999975, example: {'ranks': [{'rank': 1, 'agentID': 0}, {'rank': 2, 'agentID': 1}], 'replayFile': 'replays/488352684.json'}\n",
      "train loss in 1 epoch in 1 batch: -1.50119\n",
      "val loss in 1 epoch: -1.54367\n",
      "train loss in 2 epoch in 1 batch: -1.49973\n",
      "val loss in 2 epoch: -1.55872\n",
      "train loss in 3 epoch in 1 batch: -1.51730\n",
      "val loss in 3 epoch: -1.56882\n",
      "train loss in 4 epoch in 1 batch: -1.53521\n",
      "val loss in 4 epoch: -1.57484\n",
      "train loss in 5 epoch in 1 batch: -1.53064\n",
      "val loss in 5 epoch: -1.58003\n",
      "train loss in 6 epoch in 1 batch: -1.53041\n",
      "val loss in 6 epoch: -1.58371\n",
      "train loss in 7 epoch in 1 batch: -1.53970\n",
      "val loss in 7 epoch: -1.58579\n",
      "train loss in 8 epoch in 1 batch: -1.55306\n",
      "val loss in 8 epoch: -1.58726\n",
      "Round 8, game: 0, is_win: 0.0, max_step: 115, reward: -0.9999999989999999, example: {'ranks': [{'rank': 1, 'agentID': 1}, {'rank': 2, 'agentID': 0}], 'replayFile': 'replays/530125251.json'}\n",
      "Round 8, game: 1, is_win: 0.0, max_step: 110, reward: -0.9999999998333333, example: {'ranks': [{'rank': 1, 'agentID': 0}, {'rank': 2, 'agentID': 1}], 'replayFile': 'replays/285449043.json'}\n",
      "Round 8, game: 2, is_win: 0.0, max_step: 30, reward: -0.9999999989999999, example: {'ranks': [{'rank': 1, 'agentID': 1}, {'rank': 2, 'agentID': 0}], 'replayFile': 'replays/514030694.json'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/IPython/core/interactiveshell.py:3338: DtypeWarning: Columns (9,10,11,12,13,14,17,18,19,20,21,22) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  if (await self.run_code(code, result,  async_=asy)):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Round 8, game: 3, is_win: 0.0, max_step: 193, reward: -0.9999999996666666, example: {'ranks': [{'rank': 1, 'agentID': 0}, {'rank': 2, 'agentID': 1}], 'replayFile': 'replays/441574153.json'}\n",
      "Round 8, game: 4, is_win: 0.0, max_step: 70, reward: -0.9999999995, example: {'ranks': [{'rank': 1, 'agentID': 1}, {'rank': 2, 'agentID': 0}], 'replayFile': 'replays/627688022.json'}\n",
      "Round 8, game: 5, is_win: 0.0, max_step: 114, reward: -0.9999999995, example: {'ranks': [{'rank': 1, 'agentID': 0}, {'rank': 2, 'agentID': 1}], 'replayFile': 'replays/60817287.json'}\n",
      "Round 8, game: 6, is_win: 0.0, max_step: 115, reward: -0.9999999989999999, example: {'ranks': [{'rank': 1, 'agentID': 1}, {'rank': 2, 'agentID': 0}], 'replayFile': 'replays/592756178.json'}\n",
      "Round 8, game: 7, is_win: 0.0, max_step: 29, reward: -0.9999999995, example: {'ranks': [{'rank': 1, 'agentID': 0}, {'rank': 2, 'agentID': 1}], 'replayFile': 'replays/300371104.json'}\n",
      "train loss in 1 epoch in 1 batch: -1.54891\n",
      "val loss in 1 epoch: -1.59426\n",
      "train loss in 2 epoch in 1 batch: -1.51985\n",
      "val loss in 2 epoch: -1.59460\n",
      "train loss in 3 epoch in 1 batch: -1.54204\n",
      "val loss in 3 epoch: -1.59326\n",
      "train loss in 4 epoch in 1 batch: -1.55105\n",
      "val loss in 4 epoch: -1.59155\n",
      "train loss in 5 epoch in 1 batch: -1.54350\n",
      "val loss in 5 epoch: -1.58682\n",
      "train loss in 6 epoch in 1 batch: -1.55438\n",
      "val loss in 6 epoch: -1.58068\n",
      "train loss in 7 epoch in 1 batch: -1.54705\n",
      "val loss in 7 epoch: -1.57782\n",
      "train loss in 8 epoch in 1 batch: -1.54981\n",
      "val loss in 8 epoch: -1.57469\n",
      "Round 9, game: 0, is_win: 0.0, max_step: 149, reward: -0.9999999995, example: {'ranks': [{'rank': 1, 'agentID': 1}, {'rank': 2, 'agentID': 0}], 'replayFile': 'replays/44556670.json'}\n",
      "Round 9, game: 1, is_win: 0.0, max_step: 358, reward: -0.9999999995, example: {'ranks': [{'rank': 1, 'agentID': 0}, {'rank': 2, 'agentID': 1}], 'replayFile': 'replays/716298973.json'}\n",
      "Round 9, game: 2, is_win: 0.0, max_step: 189, reward: -0.9999999998571428, example: {'ranks': [{'rank': 1, 'agentID': 1}, {'rank': 2, 'agentID': 0}], 'replayFile': 'replays/340659617.json'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/IPython/core/interactiveshell.py:3338: DtypeWarning: Columns (26,27,28) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  if (await self.run_code(code, result,  async_=asy)):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Round 9, game: 3, is_win: 0.0, max_step: 277, reward: -0.9999999999230769, example: {'ranks': [{'rank': 1, 'agentID': 0}, {'rank': 2, 'agentID': 1}], 'replayFile': 'replays/402714107.json'}\n",
      "Round 9, game: 4, is_win: 0.0, max_step: 118, reward: -0.9999999989999999, example: {'ranks': [{'rank': 1, 'agentID': 1}, {'rank': 2, 'agentID': 0}], 'replayFile': 'replays/288759037.json'}\n",
      "Round 9, game: 5, is_win: 0.0, max_step: 29, reward: -0.9999999995, example: {'ranks': [{'rank': 1, 'agentID': 0}, {'rank': 2, 'agentID': 1}], 'replayFile': 'replays/34982697.json'}\n",
      "Round 9, game: 6, is_win: 0.0, max_step: 70, reward: -0.9999999995, example: {'ranks': [{'rank': 1, 'agentID': 1}, {'rank': 2, 'agentID': 0}], 'replayFile': 'replays/320655317.json'}\n",
      "Round 9, game: 7, is_win: 0.0, max_step: 30, reward: -0.9999999989999999, example: {'ranks': [{'rank': 1, 'agentID': 0}, {'rank': 2, 'agentID': 1}], 'replayFile': 'replays/243847411.json'}\n",
      "train loss in 1 epoch in 1 batch: -1.55535\n",
      "val loss in 1 epoch: -1.61191\n",
      "train loss in 1 epoch in 2 batch: -1.63999\n",
      "val loss in 1 epoch: -1.60824\n",
      "train loss in 2 epoch in 1 batch: -1.57716\n",
      "val loss in 2 epoch: -1.59987\n",
      "train loss in 2 epoch in 2 batch: -1.50265\n",
      "val loss in 2 epoch: -1.59841\n",
      "train loss in 3 epoch in 1 batch: -1.59535\n",
      "val loss in 3 epoch: -1.59804\n",
      "train loss in 3 epoch in 2 batch: -1.55045\n",
      "val loss in 3 epoch: -1.59896\n",
      "train loss in 4 epoch in 1 batch: -1.60795\n",
      "val loss in 4 epoch: -1.59805\n",
      "train loss in 4 epoch in 2 batch: -1.48634\n",
      "val loss in 4 epoch: -1.59956\n",
      "train loss in 5 epoch in 1 batch: -1.59090\n",
      "val loss in 5 epoch: -1.60064\n",
      "train loss in 5 epoch in 2 batch: -1.55880\n",
      "val loss in 5 epoch: -1.60220\n",
      "train loss in 6 epoch in 1 batch: -1.57009\n",
      "val loss in 6 epoch: -1.60075\n",
      "train loss in 6 epoch in 2 batch: -1.53967\n",
      "val loss in 6 epoch: -1.60124\n",
      "train loss in 7 epoch in 1 batch: -1.60453\n",
      "val loss in 7 epoch: -1.60087\n",
      "train loss in 7 epoch in 2 batch: -1.54836\n",
      "val loss in 7 epoch: -1.60320\n",
      "train loss in 8 epoch in 1 batch: -1.54433\n",
      "val loss in 8 epoch: -1.60635\n",
      "train loss in 8 epoch in 2 batch: -1.62976\n",
      "val loss in 8 epoch: -1.60878\n",
      "Round 10, game: 0, is_win: 0.0, max_step: 150, reward: -0.9999999998333333, example: {'ranks': [{'rank': 1, 'agentID': 1}, {'rank': 2, 'agentID': 0}], 'replayFile': 'replays/91571465.json'}\n",
      "Round 10, game: 1, is_win: 0.0, max_step: 69, reward: -0.9999999995, example: {'ranks': [{'rank': 1, 'agentID': 0}, {'rank': 2, 'agentID': 1}], 'replayFile': 'replays/645868022.json'}\n",
      "Round 10, game: 2, is_win: 0.0, max_step: 70, reward: -0.9999999995, example: {'ranks': [{'rank': 1, 'agentID': 1}, {'rank': 2, 'agentID': 0}], 'replayFile': 'replays/52365366.json'}\n",
      "Round 10, game: 3, is_win: 0.0, max_step: 350, reward: -0.9999999995, example: {'ranks': [{'rank': 1, 'agentID': 0}, {'rank': 2, 'agentID': 1}], 'replayFile': 'replays/179856334.json'}\n",
      "Round 10, game: 4, is_win: 0.0, max_step: 30, reward: -0.9999999989999999, example: {'ranks': [{'rank': 1, 'agentID': 1}, {'rank': 2, 'agentID': 0}], 'replayFile': 'replays/281592091.json'}\n",
      "Round 10, game: 5, is_win: 0.0, max_step: 29, reward: -0.9999999989999999, example: {'ranks': [{'rank': 1, 'agentID': 0}, {'rank': 2, 'agentID': 1}], 'replayFile': 'replays/433958989.json'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/IPython/core/interactiveshell.py:3338: DtypeWarning: Columns (26,27,28) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  if (await self.run_code(code, result,  async_=asy)):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Round 10, game: 6, is_win: 0.0, max_step: 230, reward: -0.9999999998, example: {'ranks': [{'rank': 1, 'agentID': 1}, {'rank': 2, 'agentID': 0}], 'replayFile': 'replays/50629214.json'}\n",
      "Round 10, game: 7, is_win: 0.0, max_step: 114, reward: -0.9999999995, example: {'ranks': [{'rank': 1, 'agentID': 0}, {'rank': 2, 'agentID': 1}], 'replayFile': 'replays/458918533.json'}\n",
      "train loss in 1 epoch in 1 batch: -1.54775\n",
      "val loss in 1 epoch: -1.58892\n",
      "train loss in 2 epoch in 1 batch: -1.54906\n",
      "val loss in 2 epoch: -1.59005\n",
      "train loss in 3 epoch in 1 batch: -1.55794\n",
      "val loss in 3 epoch: -1.58996\n",
      "train loss in 4 epoch in 1 batch: -1.56587\n",
      "val loss in 4 epoch: -1.59248\n",
      "train loss in 5 epoch in 1 batch: -1.56843\n",
      "val loss in 5 epoch: -1.59789\n",
      "train loss in 6 epoch in 1 batch: -1.56429\n",
      "val loss in 6 epoch: -1.59959\n",
      "train loss in 7 epoch in 1 batch: -1.55366\n",
      "val loss in 7 epoch: -1.60022\n",
      "train loss in 8 epoch in 1 batch: -1.56256\n",
      "val loss in 8 epoch: -1.59591\n",
      "Round 11, game: 0, is_win: 0.0, max_step: 189, reward: -0.99999999975, example: {'ranks': [{'rank': 1, 'agentID': 1}, {'rank': 2, 'agentID': 0}], 'replayFile': 'replays/774252441.json'}\n",
      "Round 11, game: 1, is_win: 0.0, max_step: 69, reward: -0.9999999989999999, example: {'ranks': [{'rank': 1, 'agentID': 0}, {'rank': 2, 'agentID': 1}], 'replayFile': 'replays/987689860.json'}\n",
      "Round 11, game: 2, is_win: 0.0, max_step: 70, reward: -0.9999999996666666, example: {'ranks': [{'rank': 1, 'agentID': 1}, {'rank': 2, 'agentID': 0}], 'replayFile': 'replays/151993381.json'}\n",
      "Round 11, game: 3, is_win: 0.0, max_step: 30, reward: -0.9999999989999999, example: {'ranks': [{'rank': 1, 'agentID': 0}, {'rank': 2, 'agentID': 1}], 'replayFile': 'replays/945197713.json'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/IPython/core/interactiveshell.py:3338: DtypeWarning: Columns (17,18,19,20,21,22) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  if (await self.run_code(code, result,  async_=asy)):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Round 11, game: 4, is_win: 0.0, max_step: 235, reward: -0.9999999995, example: {'ranks': [{'rank': 1, 'agentID': 1}, {'rank': 2, 'agentID': 0}], 'replayFile': 'replays/285045419.json'}\n",
      "Round 11, game: 5, is_win: 0.0, max_step: 35, reward: -0.9999999996666666, example: {'ranks': [{'rank': 1, 'agentID': 0}, {'rank': 2, 'agentID': 1}], 'replayFile': 'replays/935655093.json'}\n",
      "Round 11, game: 6, is_win: 0.0, max_step: 198, reward: -0.9999999996666666, example: {'ranks': [{'rank': 1, 'agentID': 1}, {'rank': 2, 'agentID': 0}], 'replayFile': 'replays/224986811.json'}\n",
      "Round 11, game: 7, is_win: 0.0, max_step: 30, reward: -0.9999999995, example: {'ranks': [{'rank': 1, 'agentID': 0}, {'rank': 2, 'agentID': 1}], 'replayFile': 'replays/762488942.json'}\n",
      "train loss in 1 epoch in 1 batch: -1.56545\n",
      "val loss in 1 epoch: -1.61540\n",
      "train loss in 2 epoch in 1 batch: -1.56946\n",
      "val loss in 2 epoch: -1.62082\n",
      "train loss in 3 epoch in 1 batch: -1.60239\n",
      "val loss in 3 epoch: -1.62576\n",
      "train loss in 4 epoch in 1 batch: -1.60268\n",
      "val loss in 4 epoch: -1.62637\n",
      "train loss in 5 epoch in 1 batch: -1.60063\n",
      "val loss in 5 epoch: -1.62474\n",
      "train loss in 6 epoch in 1 batch: -1.59921\n",
      "val loss in 6 epoch: -1.62143\n",
      "train loss in 7 epoch in 1 batch: -1.59011\n",
      "val loss in 7 epoch: -1.61863\n",
      "train loss in 8 epoch in 1 batch: -1.58550\n",
      "val loss in 8 epoch: -1.61696\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/IPython/core/interactiveshell.py:3338: DtypeWarning: Columns (9,10,11,12,13,14) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  if (await self.run_code(code, result,  async_=asy)):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Round 12, game: 0, is_win: 0.0, max_step: 314, reward: -0.9999999999333333, example: {'ranks': [{'rank': 1, 'agentID': 1}, {'rank': 2, 'agentID': 0}], 'replayFile': 'replays/662124363.json'}\n",
      "Round 12, game: 1, is_win: 0.0, max_step: 154, reward: -0.9999999989999999, example: {'ranks': [{'rank': 1, 'agentID': 0}, {'rank': 2, 'agentID': 1}], 'replayFile': 'replays/227425080.json'}\n",
      "Round 12, game: 2, is_win: 0.0, max_step: 70, reward: -0.9999999989999999, example: {'ranks': [{'rank': 1, 'agentID': 1}, {'rank': 2, 'agentID': 0}], 'replayFile': 'replays/75057355.json'}\n",
      "Round 12, game: 3, is_win: 0.0, max_step: 358, reward: -0.9999999998, example: {'ranks': [{'rank': 1, 'agentID': 0}, {'rank': 2, 'agentID': 1}], 'replayFile': 'replays/169335405.json'}\n",
      "Round 12, game: 4, is_win: 0.0, max_step: 29, reward: -0.9999999989999999, example: {'ranks': [{'rank': 1, 'agentID': 1}, {'rank': 2, 'agentID': 0}], 'replayFile': 'replays/215478765.json'}\n",
      "Round 12, game: 5, is_win: 0.0, max_step: 110, reward: -0.9999999995, example: {'ranks': [{'rank': 1, 'agentID': 0}, {'rank': 2, 'agentID': 1}], 'replayFile': 'replays/2992754.json'}\n",
      "Round 12, game: 6, is_win: 0.0, max_step: 150, reward: -0.9999999996666666, example: {'ranks': [{'rank': 1, 'agentID': 1}, {'rank': 2, 'agentID': 0}], 'replayFile': 'replays/608502765.json'}\n",
      "Round 12, game: 7, is_win: 0.0, max_step: 110, reward: -0.9999999989999999, example: {'ranks': [{'rank': 1, 'agentID': 0}, {'rank': 2, 'agentID': 1}], 'replayFile': 'replays/832673289.json'}\n",
      "train loss in 1 epoch in 1 batch: -1.54190\n",
      "val loss in 1 epoch: -1.55184\n",
      "train loss in 1 epoch in 2 batch: -1.51120\n",
      "val loss in 1 epoch: -1.56112\n",
      "train loss in 2 epoch in 1 batch: -1.53063\n",
      "val loss in 2 epoch: -1.56963\n",
      "train loss in 2 epoch in 2 batch: -1.60806\n",
      "val loss in 2 epoch: -1.57635\n",
      "train loss in 3 epoch in 1 batch: -1.51544\n",
      "val loss in 3 epoch: -1.58293\n",
      "train loss in 3 epoch in 2 batch: -1.66197\n",
      "val loss in 3 epoch: -1.58711\n",
      "train loss in 4 epoch in 1 batch: -1.56379\n",
      "val loss in 4 epoch: -1.58884\n",
      "train loss in 4 epoch in 2 batch: -1.61484\n",
      "val loss in 4 epoch: -1.58637\n",
      "train loss in 5 epoch in 1 batch: -1.56719\n",
      "val loss in 5 epoch: -1.58152\n",
      "train loss in 5 epoch in 2 batch: -1.58433\n",
      "val loss in 5 epoch: -1.57768\n",
      "train loss in 6 epoch in 1 batch: -1.51683\n",
      "val loss in 6 epoch: -1.57665\n",
      "train loss in 6 epoch in 2 batch: -1.65869\n",
      "val loss in 6 epoch: -1.57482\n",
      "train loss in 7 epoch in 1 batch: -1.56441\n",
      "val loss in 7 epoch: -1.57314\n",
      "train loss in 7 epoch in 2 batch: -1.56797\n",
      "val loss in 7 epoch: -1.57410\n",
      "train loss in 8 epoch in 1 batch: -1.55608\n",
      "val loss in 8 epoch: -1.57629\n",
      "train loss in 8 epoch in 2 batch: -1.58055\n",
      "val loss in 8 epoch: -1.57929\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/IPython/core/interactiveshell.py:3338: DtypeWarning: Columns (26,27,28) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  if (await self.run_code(code, result,  async_=asy)):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Round 13, game: 0, is_win: 0.0, max_step: 118, reward: -0.9999999998333333, example: {'ranks': [{'rank': 1, 'agentID': 1}, {'rank': 2, 'agentID': 0}], 'replayFile': 'replays/118980946.json'}\n",
      "Round 13, game: 1, is_win: 0.0, max_step: 30, reward: -0.9999999995, example: {'ranks': [{'rank': 1, 'agentID': 0}, {'rank': 2, 'agentID': 1}], 'replayFile': 'replays/772441077.json'}\n",
      "Round 13, game: 2, is_win: 0.0, max_step: 110, reward: -0.9999999996666666, example: {'ranks': [{'rank': 1, 'agentID': 1}, {'rank': 2, 'agentID': 0}], 'replayFile': 'replays/419108120.json'}\n",
      "Round 13, game: 3, is_win: 0.0, max_step: 150, reward: -0.9999999999, example: {'ranks': [{'rank': 1, 'agentID': 0}, {'rank': 2, 'agentID': 1}], 'replayFile': 'replays/534433658.json'}\n",
      "Round 13, game: 4, is_win: 0.0, max_step: 278, reward: -0.9999999989999999, example: {'ranks': [{'rank': 1, 'agentID': 1}, {'rank': 2, 'agentID': 0}], 'replayFile': 'replays/865398223.json'}\n",
      "Round 13, game: 5, is_win: 0.0, max_step: 70, reward: -0.9999999995, example: {'ranks': [{'rank': 1, 'agentID': 0}, {'rank': 2, 'agentID': 1}], 'replayFile': 'replays/507292382.json'}\n",
      "Round 13, game: 6, is_win: 0.0, max_step: 115, reward: -0.9999999989999999, example: {'ranks': [{'rank': 1, 'agentID': 1}, {'rank': 2, 'agentID': 0}], 'replayFile': 'replays/893721773.json'}\n"
     ]
    }
   ],
   "source": [
    "t = 0  #  1778 - value_iter\n",
    "B = 8\n",
    "\n",
    "model = NNWithCustomFeatures(83, 0.05, 64)\n",
    "\n",
    "writer = SummaryWriter()\n",
    "\n",
    "#optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "iter_data = {\"n_iter\": 0}\n",
    "\n",
    "while True:\n",
    "    t += 1\n",
    "    np.random.seed(t)\n",
    "    torch.save(model.state_dict(), '../submissions/simple/models/ac_last')\n",
    "    trainD = []\n",
    "    valD = []\n",
    "    for i in np.arange(B):\n",
    "        seed = t * B + i\n",
    "#         _f = str(seed) + \".txt\"\n",
    "        _f = \"log_{}.txt\".format(i)\n",
    "        bot = build_runnable_bot_with_flags({\n",
    "            \"model_path\": \"models/ac_last\",\n",
    "            \"use_policy\": True,\n",
    "            \"is_neural\": True,\n",
    "            \"prob_use_default_agent\": 0.0,\n",
    "            \"prob_use_random\": 0.05,\n",
    "            \"ohe_path\": \"models/ohe_v2\",\n",
    "            \"use_old_units_cargo_rules\": False,\n",
    "            \"log_features_path\": \"../../research/features_iter/\", \"log_path_file_name\": _f\n",
    "        })\n",
    "        if i % 2 == 0:\n",
    "            _r = run_game(bot, simple_bot, loglevel=0, seed=seed)\n",
    "        else:\n",
    "            _r = run_game(simple_bot, bot, loglevel=0, seed=seed)\n",
    "        wins = np.mean(count_series([_r]))\n",
    "        if i % 2 == 1:\n",
    "            wins = 1 - wins\n",
    "        game_set = dataset.get_dataset_from_file(os.path.join(\"features_iter/\", _f), wins)\n",
    "        reward = np.sum(game_set.weights) / (np.sum(game_set.weights != 0) + 1e-9)\n",
    "        trainD_ohe, valD_ohe, _ = prepare_features(game_set, game_set, OHE)\n",
    "        max_step = np.max(game_set.features[:, 31])\n",
    "        trainD_ohe_with_next = add_next_features(trainD_ohe)\n",
    "        trainD_ohe_with_next_sampled = sample_dataset(trainD_ohe_with_next, 0.1)\n",
    "        print(\"Round {}, game: {}, is_win: {}, max_step: {}, reward: {}, example: {}\".format(t, i, wins, max_step, reward, _r[0]['results']))\n",
    "        writer.add_scalar('data/reward', reward, seed)\n",
    "        writer.add_scalar('data/winrate', wins, seed)\n",
    "        writer.add_scalar('data/max_step', max_step, seed)\n",
    "        trainD.append(trainD_ohe_with_next_sampled)\n",
    "        valD.append(trainD_ohe_with_next_sampled)\n",
    "    trainD = dataset.concat_datasets(trainD)\n",
    "    valD = dataset.concat_datasets(valD)\n",
    "    try:\n",
    "        learn(trainD, valD, model, actor_critic_loss, iter_data=iter_data,\n",
    "              batch_size=64, epochs=8, freq=1, writer=writer, lr=1e-4)\n",
    "    except Exception as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
