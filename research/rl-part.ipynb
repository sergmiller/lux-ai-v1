{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "import json\n",
    "import os\n",
    "\n",
    "import subprocess\n",
    "\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%config Completer.use_jedi = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "\n",
    "from tensorboardX import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 702,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'utils.dataset' from '/Users/sergmiller/Documents/my/lux-ai-v1/research/utils/dataset.py'>"
      ]
     },
     "execution_count": 702,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from importlib import reload\n",
    "from utils import dataset\n",
    "\n",
    "reload(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 705,
   "metadata": {},
   "outputs": [],
   "source": [
    "def learn(train, val, model_ff, criterion, iter_data, epochs=5, batch_size=64, shuffle=True, freq=10,lr=1e-3, l2=1e-5, optimizer=None, writer=None): \n",
    "    assert iter_data is not None\n",
    "    if writer is None:\n",
    "        writer = SummaryWriter()\n",
    "    \n",
    "#     np.random.seed(1)\n",
    "    ids_nn = np.arange(train.targets.shape[0])\n",
    "    \n",
    "    reshape_to_last = lambda x: torch.reshape(x, [np.prod(x.shape[:-1]), x.shape[-1]])\n",
    "\n",
    "    if optimizer is None:\n",
    "        optimizer = optim.Adam(model_ff.parameters(), lr=lr, weight_decay=l2)\n",
    "\n",
    "    time_for_print_loss = lambda i: (i + 1) % freq == 0\n",
    "    \n",
    "    n_iter = 0\n",
    "\n",
    "\n",
    "    for epoch in np.arange(epochs):\n",
    "        np.random.shuffle(ids_nn)\n",
    "\n",
    "        model_ff.train(True)\n",
    "\n",
    "        for b in np.arange(0, train.targets.shape[0], batch_size):\n",
    "            X_batch = torch.FloatTensor(train.features[ids_nn[b:b+batch_size]])\n",
    "            y_batch = torch.FloatTensor(train.weights[ids_nn[b:b+batch_size]])  # reward(advantage)\n",
    "            a_batch = torch.LongTensor(train.targets[ids_nn[b:b+batch_size]])  # action\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            y_pred_logits = model_ff(X_batch)\n",
    "\n",
    "            loss = criterion(y_pred_logits, y_batch, a_batch, X_batch)\n",
    "            loss.backward()\n",
    "\n",
    "            optimizer.step()\n",
    "\n",
    "            if (b // batch_size + 1) % freq == 0:\n",
    "                print('train loss in %d epoch in %d batch: %.5f' %\n",
    "                  (epoch + 1, b // batch_size + 1, loss.item()))\n",
    "\n",
    "                writer.add_scalar('data/train_loss', loss.item(), iter_data[\"n_iter\"])\n",
    "                writer.add_scalar('data/epoch', epoch + 1, iter_data[\"n_iter\"])\n",
    "                writer.add_scalar('data/batch', b // batch_size + 1, iter_data[\"n_iter\"])\n",
    "\n",
    "                val_loss = 0\n",
    "                its = 0\n",
    "                model_ff.train(False)\n",
    "                for b in np.arange(0, val.targets.shape[0], batch_size):\n",
    "                    its += 1\n",
    "                    X_batch = torch.FloatTensor(val.features[b:b+batch_size])\n",
    "#                     X_batch = reshape_to_last(X_batch)\n",
    "\n",
    "                    y_batch = torch.FloatTensor(val.weights[b:b+batch_size])\n",
    "                    a_batch = torch.LongTensor(val.targets[b:b+batch_size])\n",
    "                    with torch.no_grad():\n",
    "                        y_pred_logits = model_ff(X_batch)\n",
    "                    loss = criterion(y_pred_logits, y_batch, a_batch, X_batch)\n",
    "                    val_loss += loss.item()\n",
    "                val_loss /= its\n",
    "                print('val loss in %d epoch: %.5f' % (epoch + 1, val_loss))\n",
    "\n",
    "                writer.add_scalar('data/val_loss', val_loss, iter_data[\"n_iter\"])\n",
    "                n_iter += 1\n",
    "                iter_data[\"n_iter\"] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# datasets = dataset.read_datasets_from_dir(\"features_v3/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 'cargo_vol_total'),\n",
       " (1, 'cargo_fuel_total'),\n",
       " (2, 'unit_can_build'),\n",
       " (3, 'unit_routine'),\n",
       " (4, 'unit_last_action'),\n",
       " (5, 'near_city_dist'),\n",
       " (6, 'near_city_dir'),\n",
       " (7, 'near_city_fuel'),\n",
       " (8, 'near_city_light_upkeep'),\n",
       " (9, 'city_size'),\n",
       " (10, 'opp_near_city_dist'),\n",
       " (11, 'opp_near_city_dir'),\n",
       " (12, 'opp_near_city_fuel'),\n",
       " (13, 'opp_near_city_light_upkeep'),\n",
       " (14, 'opp_city_size'),\n",
       " (15, 'near_resource_dist'),\n",
       " (16, 'near_resource_dir'),\n",
       " (17, 'near_resource_type'),\n",
       " (18, 'near_resource_amount'),\n",
       " (19, 'my_city_count'),\n",
       " (20, 'opp_city_count'),\n",
       " (21, 'turn'),\n",
       " (22, 'is_night'),\n",
       " (23, 'time_to_night'),\n",
       " (24, 'width'),\n",
       " (25, 'height'),\n",
       " (26, 'my_research'),\n",
       " (27, 'opp_research'),\n",
       " (28, 'my_research_coal'),\n",
       " (29, 'opp_research_coal'),\n",
       " (30, 'my_research_uran'),\n",
       " (31, 'opp_research_uran'),\n",
       " (32, 'action'),\n",
       " (33, 'my_tiles'),\n",
       " (34, 'opp_tiles')]"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dataset.read_columns_from_random_file(\"features_v3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 3, 4, 6, 11, 16, 17, 22, 28, 29, 30, 31]"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.CAT_FEATURES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 409,
   "metadata": {},
   "outputs": [],
   "source": [
    "FLOAT_FEATURES = [i for i in range(42 + 32*32*7) if i not in dataset.CAT_FEATURES_V4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([0, 1, 5, 6, 7, 9, 10, 11, 12, 13, 15, 17, 18, 19, 20, 21, 23, 26, 27, 28],\n",
       " [7200, 7201, 7202, 7203, 7204, 7205, 7206, 7207, 7208, 7209])"
      ]
     },
     "execution_count": 410,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "FLOAT_FEATURES[:20], FLOAT_FEATURES[-10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../submissions/simple/models/ohe_v2\", \"rb\") as f:\n",
    "    OHE = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 419,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_features(t: dataset.Dataset, v: dataset.Dataset, ohe=None, categories=None) -> (dataset.Dataset, dataset.Dataset):\n",
    "    create_ohe = ohe is None\n",
    "    if create_ohe:\n",
    "         ohe = OneHotEncoder(sparse=False, categories=categories)\n",
    "    def prepare(d, is_train):\n",
    "        cf = d.features[:, dataset.CAT_FEATURES_V4]\n",
    "        ff = d.features[:, FLOAT_FEATURES]\n",
    "        cf[cf == \"False\"] = False\n",
    "        cf[cf == \"True\"] = True\n",
    "        cf[cf == None] = \"None\"\n",
    "        cf[cf == \"1\"] = 1\n",
    "        cf[cf == \"2\"] = 2\n",
    "        cf[cf == \"3\"] = 3\n",
    "        ff[ff == \"None\"] = 0\n",
    "        cf_o = ohe.fit_transform(cf) if is_train and create_ohe else ohe.transform(cf)\n",
    "        return dataset.Dataset(\n",
    "            features=np.array(np.concatenate([cf_o, ff], axis=1), dtype=np.float),\n",
    "            targets=np.array(d.targets, dtype=np.float),\n",
    "            weights=np.array(d.weights, dtype=np.float),\n",
    "            next_state_id = d.next_state_id\n",
    "        )\n",
    "    t = prepare(t, True)\n",
    "    v = prepare(v, False)\n",
    "    return (t,v, ohe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dt,dv,OHE = prepare_features(data, data, None, [\n",
    "#      np.array([\"None\", False, True], dtype=object),\n",
    "#      np.array([1, 2, 3], dtype=object),\n",
    "#      np.array(['None', 'bcity', 'e', 'n', 'p', 's', 'w'], dtype=object),\n",
    "#      np.array(['None', 'c', 'e', 'n', 's', 'w'], dtype=object),\n",
    "#      np.array([\"None\", False, True], dtype=object),\n",
    "#      np.array(['None', 'e', 'n', 's', 'w'], dtype=object),\n",
    "#      np.array([\"None\", False, True], dtype=object),\n",
    "#      np.array(['None', 'c', 'e', 'n', 's', 'w'], dtype=object),\n",
    "#      np.array(['None', 'coal', 'uranium', 'wood'], dtype=object),\n",
    "#      np.array([\"None\", False, True], dtype=object),\n",
    "#      np.array([\"None\", False, True], dtype=object),\n",
    "#      np.array([\"None\", False, True], dtype=object),\n",
    "#      np.array([\"None\", False, True], dtype=object),\n",
    "#      np.array([\"None\", False, True], dtype=object)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array(['None', False, True], dtype=object),\n",
       " array([1, 2, 3], dtype=object),\n",
       " array(['None', 'bcity', 'e', 'n', 'p', 's', 'w'], dtype=object),\n",
       " array(['None', 'c', 'e', 'n', 's', 'w'], dtype=object),\n",
       " array(['None', False, True], dtype=object),\n",
       " array(['None', 'e', 'n', 's', 'w'], dtype=object),\n",
       " array(['None', False, True], dtype=object),\n",
       " array(['None', 'c', 'e', 'n', 's', 'w'], dtype=object),\n",
       " array(['None', 'coal', 'uranium', 'wood'], dtype=object),\n",
       " array(['None', False, True], dtype=object),\n",
       " array(['None', False, True], dtype=object),\n",
       " array(['None', False, True], dtype=object),\n",
       " array(['None', False, True], dtype=object),\n",
       " array(['None', False, True], dtype=object)]"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "OHE.categories_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(\"../submissions/simple/models/ohe_v2\", \"wb\") as f:\n",
    "#     pickle.dump(OHE, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(\"../submissions/simple/models/ohe_v1\", \"wb\") as f:\n",
    "#     pickle.dump(ohe, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 626,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAP_F = 32 * 32 * 7\n",
    "\n",
    "class NNWithCustomFeatures(nn.Module):\n",
    "    def __init__(self, INPUT_F, DROP_P, H, A=6):\n",
    "        super().__init__()\n",
    "        INPUT_F_C = INPUT_F + 128\n",
    "        self.model_q =  nn.Sequential(\n",
    "            nn.Dropout(DROP_P),\n",
    "            nn.Linear(INPUT_F_C, H),\n",
    "            nn.LayerNorm(H),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(DROP_P),\n",
    "            nn.Linear(H, H),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(DROP_P),\n",
    "            nn.Linear(H, H),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(H, A),\n",
    "            nn.ReLU()\n",
    "#             nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "        self.model_p =  nn.Sequential(\n",
    "            nn.Dropout(DROP_P),\n",
    "            nn.Linear(INPUT_F_C, H),\n",
    "            nn.LayerNorm(H),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(DROP_P),\n",
    "            nn.Linear(H, H),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(DROP_P),\n",
    "            nn.Linear(H, H),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(H, A)\n",
    "#             nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "        self.map_model = nn.Sequential(\n",
    "            nn.Conv2d(7, 64, 3),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2),  # after -> (16,16)\n",
    "            nn.Conv2d(64, 128, 3),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2),  # after -> (8, 8)\n",
    "            nn.Conv2d(128, 256, 3),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2),  # after -> (4, 4)\n",
    "#             nn.Conv2d(128, 256, 3),\n",
    "#             nn.ReLU(inplace=True),\n",
    "#             nn.MaxPool2d(2),  # after -> (1, 1)\n",
    "        )\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((4, 4))\n",
    "        self.proj = nn.Sequential(\n",
    "            nn.Dropout(p=DROP_P),\n",
    "            nn.Linear(256 * 4 * 4, 256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(p=DROP_P),\n",
    "            nn.Linear(256, 128)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        L = x.shape[1]\n",
    "        cur_r = self.forward_impl(x[:, :L // 2])\n",
    "        next_r =  self.forward_impl(x[:, L // 2:])\n",
    "        return torch.cat([cur_r, next_r],dim=1)\n",
    "\n",
    "    def forward_impl(self, x):\n",
    "        mapp = x[:, -MAP_F:].reshape(-1, 32, 32, 7)\n",
    "        rest = x[:, :-MAP_F]\n",
    "        mapp = torch.transpose(mapp, 1, -1)\n",
    "        mapp = self.avgpool(self.map_model(mapp))\n",
    "        mapp = torch.flatten(mapp, 1)\n",
    "        mapp_f = self.proj(mapp)\n",
    "#         print(mapp_f.shape)\n",
    "        input_x = torch.cat([rest, mapp_f], dim=1)\n",
    "#         print(input_x.shape)\n",
    "#         return self.model_q(input_x)\n",
    "        return torch.cat([self.model_q(input_x), self.model_p(input_x)], dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 627,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NNWithCustomFeatures(63, 0.05, 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 630,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0000,  0.0000,  0.1109,  0.0476,  0.0000,  0.0915, -0.0937, -0.0995,\n",
       "         -0.1064,  0.0649,  0.0547, -0.0450],\n",
       "        [ 0.0000,  0.0000,  0.1094,  0.0437,  0.0000,  0.0924, -0.0958, -0.0997,\n",
       "         -0.1076,  0.0741,  0.0568, -0.0414],\n",
       "        [ 0.0000,  0.0000,  0.1109,  0.0476,  0.0000,  0.0915, -0.0927, -0.0949,\n",
       "         -0.1044,  0.0684,  0.0588, -0.0371],\n",
       "        [ 0.0000,  0.0000,  0.1147,  0.0556,  0.0000,  0.0966, -0.0937, -0.0995,\n",
       "         -0.1064,  0.0649,  0.0547, -0.0450]], grad_fn=<CatBackward>)"
      ]
     },
     "execution_count": 630,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.forward_impl(torch.Tensor(4, 63 + 32*32*7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 700,
   "metadata": {},
   "outputs": [],
   "source": [
    "ENTROPY_REG = 1e-2\n",
    "def policy_loss(pi_logits, reward_batch, a_batch, X_batch):\n",
    "    pi_probs = torch.nn.Softmax(dim=1)(pi_logits)\n",
    "    return torch.mean(torch.nn.CrossEntropyLoss(reduction='none')(pi_logits, a_batch) * reward_batch \n",
    "                      - torch.sum(pi_probs * torch.log(pi_probs) * ENTROPY_REG, dim=1))\n",
    "\n",
    "def q_loss(q_vals, reward_batch, a_batch, X_batch):\n",
    "    q_vals_per_reward = q_vals[np.arange(q_vals.shape[0]), a_batch]\n",
    "    return torch.nn.MSELoss()(q_vals_per_reward, reward_batch) * 0.01\n",
    "\n",
    "gamma = 0.99\n",
    "\n",
    "def get_is_last_state(x):\n",
    "    t = torch.sum(torch.isclose(x, torch.ones_like(x) * (-1)), dim=1) == x.shape[1]\n",
    "    return t.float()\n",
    "\n",
    "def q_loss_pair(q_vals_cur_and_next, reward_batch, a_batch, X_batch):\n",
    "    q_vals = q_vals_cur_and_next[:, :6]\n",
    "    q_vals_next = q_vals_cur_and_next[:, 6:12]\n",
    "    q_vals_per_reward_cur = q_vals[np.arange(q_vals.shape[0]), a_batch]\n",
    "    X_batch_next = X_batch[:, X_batch.shape[1] // 2:]\n",
    "    best_q_vals_next = torch.max(q_vals_next,dim=1)[0] * (1 - get_is_last_state(X_batch_next))\n",
    "#     print(list(enumerate([q_vals_per_reward_cur, reward_batch, best_q_vals_next, q_vals_next])))\n",
    "#     print(0.99 * best_q_vals_next)\n",
    "#     return torch.nn.MSELoss()(target=q_vals_per_reward_cur.detach(), input=reward_batch + gamma * best_q_vals_next)\n",
    "    return torch.nn.SmoothL1Loss()(target=q_vals_per_reward_cur.detach(), input=reward_batch + gamma * best_q_vals_next)\n",
    "\n",
    "\n",
    "def actor_critic_loss(q_pi_payload, reward_batch, a_batch, X_batch):\n",
    "    q_vals = q_pi_payload[:, :6]\n",
    "    pi_logits =  q_pi_payload[:, 6:12]\n",
    "    pi_probs = torch.nn.Softmax(dim=1)(pi_logits)\n",
    "    q_vals_next = q_pi_payload[:, 12:18]\n",
    "    q_vals_per_reward_cur = q_vals[np.arange(q_vals.shape[0]), a_batch]\n",
    "    X_batch_next = X_batch[:, X_batch.shape[1] // 2:]\n",
    "    best_q_vals_next = torch.max(q_vals_next,dim=1)[0] * (1 - get_is_last_state(X_batch_next))\n",
    "    advantage = reward_batch + gamma * best_q_vals_next - q_vals_per_reward_cur\n",
    "    q_loss = torch.nn.SmoothL1Loss()(target=reward_batch + gamma * best_q_vals_next.detach(), input=q_vals_per_reward_cur)\n",
    "    pi_loss =  torch.mean(torch.nn.CrossEntropyLoss(reduction='none')(pi_logits, a_batch) * advantage.detach()\n",
    "                      + torch.sum(pi_probs * torch.log(pi_probs) * ENTROPY_REG, dim=1))\n",
    "#     print(\"q_loss={}, pi_loss={}\".format(q_loss.item(), pi_loss.item()))\n",
    "    return q_loss + pi_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 429,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_bot = \"../submissions/simple/main.py\"\n",
    "replays = \"replays\"\n",
    "\n",
    "def run_game(left_bot=simple_bot, right_bot=simple_bot, seed=42, loglevel=2):\n",
    "    replay_path = \"replay.json\"\n",
    "    python_v = \"python3.7\"\n",
    "    \n",
    "    replay_path = os.path.join(replays, str(np.random.randint(1e9)) + \".json\")\n",
    "    \n",
    "    size = np.random.choice([12,16,24,32], size=1)[0]\n",
    "    \n",
    "    res = subprocess.run([\n",
    "        \"lux-ai-2021\",\n",
    "        left_bot,\n",
    "        right_bot,\n",
    "#         \"--statefulReplay\",\n",
    "        \"--width={}\".format(size),\n",
    "        \"--height={}\".format(size),\n",
    "        \"--loglevel={}\".format(loglevel),\n",
    "        \"--python={}\".format(python_v),\n",
    "        \"--maxtime=100000\",\n",
    "        \"--maxConcurrentMatches=1\",\n",
    "        \"--seed={}\".format(seed),\n",
    "        \"--out={}\".format(replay_path)], stdout=subprocess.PIPE)\n",
    "    \n",
    "    if loglevel > 0:\n",
    "        print(res.stdout.decode())\n",
    "\n",
    "    assert res.returncode == 0\n",
    "\n",
    "    with open(replay_path, \"r\") as f:\n",
    "        result = json.load(f)\n",
    "    return result, res.stdout.decode()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 432,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "run_game(simple_bot, simple_bot)  # <-- test run one game with default bot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hashlib\n",
    "\n",
    "def build_runnable_bot_with_flags(flags: dict, origin = simple_bot, base_path = '../submissions/simple/') -> str:\n",
    "    lines = []\n",
    "    with open(origin, \"r\") as f:\n",
    "        for line in f:\n",
    "            lines.append(line[:-1])\n",
    "    text = '\\n'.join(lines)\n",
    "    f = json.dumps(flags)\n",
    "    text = text.format(f)\n",
    "    h = int(hashlib.sha256(f.encode('utf-8')).hexdigest(), 16) % (10 ** 18)\n",
    "    path = base_path + \"main_\" + str(h) + \".py\"\n",
    "    with open(path, \"w\") as f:\n",
    "        f.write(text)\n",
    "    return path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_series(results: list):\n",
    "    wins = []\n",
    "    for i, r in enumerate(results):\n",
    "        ranks = r[0]['results']['ranks']\n",
    "        teams = r[0]['teamDetails']\n",
    "        if ranks[0]['rank'] == 1 and ranks[1]['rank'] == 2:\n",
    "            if ranks[0][\"agentID\"] == i % 2:\n",
    "                wins.append(1)\n",
    "            else:\n",
    "                wins.append(0)\n",
    "        else:\n",
    "            wins.append(0.5)\n",
    "    return wins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib import Parallel, delayed\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 614,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_dataset(d, p=0.5):\n",
    "    N = len(d.features)\n",
    "    ids = np.random.choice(N, size=int(N * p))\n",
    "    return dataset.Dataset(features = d.features[ids], weights = d.weights[ids], targets = d.targets[ids])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 492,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_next_features(d):\n",
    "    assert d.next_state_id is not None\n",
    "    coupled_features = []\n",
    "    weights = []\n",
    "    targets = []\n",
    "    for i in np.arange(d.features.shape[0]):\n",
    "        next_i = d.next_state_id[i]\n",
    "        if d.next_state_id[i] != -1:\n",
    "            next_f = d.features[next_i]\n",
    "        else:\n",
    "            next_f = np.ones_like(d.features[i]) * (-1)\n",
    "        coupled_features.append(np.concatenate([d.features[i], next_f]))\n",
    "        weights.append(d.weights[i])\n",
    "        targets.append(d.targets[i])\n",
    "    return dataset.Dataset(\n",
    "        features=np.array(coupled_features),\n",
    "        weights=np.array(weights),\n",
    "        targets=np.array(targets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/IPython/core/interactiveshell.py:3338: DtypeWarning: Columns (9,10,11,12,13,14) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  if (await self.run_code(code, result,  async_=asy)):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Round 1, winrate: 0.0, max_step: 358, reward: -0.06, example: {'ranks': [{'rank': 1, 'agentID': 0}, {'rank': 2, 'agentID': 1}], 'replayFile': 'replays/717354021.json'}\n",
      "train loss in 1 epoch in 1 batch: 0.00737\n",
      "val loss in 1 epoch: -0.01027\n",
      "Round 2, winrate: 0.0, max_step: 359, reward: -0.36, example: {'ranks': [{'rank': 1, 'agentID': 1}, {'rank': 2, 'agentID': 0}], 'replayFile': 'replays/798842024.json'}\n",
      "train loss in 1 epoch in 1 batch: -0.01042\n",
      "val loss in 1 epoch: -0.01779\n",
      "train loss in 1 epoch in 2 batch: -0.01741\n",
      "val loss in 1 epoch: -0.01817\n",
      "train loss in 1 epoch in 3 batch: -0.01785\n",
      "val loss in 1 epoch: -0.01820\n",
      "train loss in 1 epoch in 4 batch: -0.01785\n",
      "val loss in 1 epoch: -0.01822\n",
      "train loss in 1 epoch in 5 batch: -0.01788\n",
      "val loss in 1 epoch: -0.01824\n",
      "Round 3, winrate: 0.0, max_step: 190, reward: -0.25, example: {'ranks': [{'rank': 1, 'agentID': 0}, {'rank': 2, 'agentID': 1}], 'replayFile': 'replays/218175338.json'}\n",
      "train loss in 1 epoch in 1 batch: -0.00846\n",
      "val loss in 1 epoch: -0.02190\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/IPython/core/interactiveshell.py:3338: DtypeWarning: Columns (9,10,11,12,13,14,17,18,19,20,21,22) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  if (await self.run_code(code, result,  async_=asy)):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Round 4, winrate: 0.0, max_step: 275, reward: -0.09, example: {'ranks': [{'rank': 1, 'agentID': 1}, {'rank': 2, 'agentID': 0}], 'replayFile': 'replays/932136058.json'}\n",
      "train loss in 1 epoch in 1 batch: -0.01735\n",
      "val loss in 1 epoch: -0.01838\n",
      "Round 5, winrate: 0.0, max_step: 192, reward: -0.09, example: {'ranks': [{'rank': 1, 'agentID': 0}, {'rank': 2, 'agentID': 1}], 'replayFile': 'replays/953453411.json'}\n",
      "train loss in 1 epoch in 1 batch: -0.03423\n",
      "val loss in 1 epoch: -0.02014\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/IPython/core/interactiveshell.py:3338: DtypeWarning: Columns (9,10,11,12,13,14) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  if (await self.run_code(code, result,  async_=asy)):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Round 6, winrate: 0.0, max_step: 359, reward: -0.01, example: {'ranks': [{'rank': 1, 'agentID': 1}, {'rank': 2, 'agentID': 0}], 'replayFile': 'replays/613579658.json'}\n",
      "train loss in 1 epoch in 1 batch: -0.01778\n",
      "val loss in 1 epoch: -0.01794\n",
      "train loss in 1 epoch in 2 batch: -0.01777\n",
      "val loss in 1 epoch: -0.01796\n",
      "train loss in 1 epoch in 3 batch: -0.01779\n",
      "val loss in 1 epoch: -0.01799\n"
     ]
    }
   ],
   "source": [
    "t = 0  #  1778 - value_iter\n",
    "B = 1\n",
    "\n",
    "model = NNWithCustomFeatures(83, 0.05, 128)\n",
    "\n",
    "writer = SummaryWriter()\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3, weight_decay=1e-5)\n",
    "\n",
    "iter_data = {\"n_iter\": 0}\n",
    "\n",
    "while True:\n",
    "    t += 1\n",
    "    np.random.seed(t)\n",
    "    torch.save(model.state_dict(), '../submissions/simple/models/ac_iter_v{}'.format(t))\n",
    "    r = []\n",
    "    for i in np.arange(B):\n",
    "        seed = t * B + i\n",
    "#         _f = str(seed) + \".txt\"\n",
    "        _f = \"log.txt\"\n",
    "        bot = build_runnable_bot_with_flags({\n",
    "            \"model_path\": \"models/ac_iter_v{}\".format(t),\n",
    "            \"use_policy\": True,\n",
    "            \"is_neural\": True,\n",
    "            \"prob_use_default_agent\": 0.5 / np.log(t + 1),\n",
    "            \"prob_use_random\": 0.05,\n",
    "            \"ohe_path\": \"models/ohe_v2\",\n",
    "            \"use_old_units_cargo_rules\": False,\n",
    "            \"log_features_path\": \"../../research/features_iter/\", \"log_path_file_name\": _f\n",
    "        })\n",
    "        if t % 2 == 0:\n",
    "            _r = run_game(bot, simple_bot, loglevel=0, seed=seed)\n",
    "        else:\n",
    "            _r = run_game(simple_bot, bot, loglevel=0, seed=seed)\n",
    "        r.append(_r)\n",
    "    wins = np.mean(count_series(r))\n",
    "    if t % 2 == 1:\n",
    "        wins = 1 - wins\n",
    "    trainD = dataset.get_dataset_from_file(os.path.join(\"features_iter/\", _f), wins)\n",
    "    reward = np.sum(trainD.weights)\n",
    "    trainD_ohe, valD_ohe, _ = prepare_features(trainD, trainD, OHE)\n",
    "    max_step = np.max(trainD.features[:, 31])\n",
    "    trainD_ohe_with_next = add_next_features(trainD_ohe)\n",
    "    valD_ohe_with_next = add_next_features(valD_ohe)\n",
    "    trainD_ohe_with_next_sampled = sample_dataset(trainD_ohe_with_next, 0.1)\n",
    "    print(\"Round {}, winrate: {}, max_step: {}, reward: {}, example: {}\".format(t, wins, max_step, reward, r[0][0]['results']))\n",
    "    writer.add_scalar('data/reward', reward, t)\n",
    "    writer.add_scalar('data/winrate', wins, t)\n",
    "    writer.add_scalar('data/max_step', max_step, t)\n",
    "    try:\n",
    "        learn(trainD_ohe_with_next_sampled, valD_ohe_with_next, model, actor_critic_loss, iter_data=iter_data,\n",
    "              batch_size=64, epochs=1, freq=1, optimizer=optimizer, writer=writer)\n",
    "    except Exception as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
